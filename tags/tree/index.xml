<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tree on A Song of Numbers and Graphs</title>
    <link>https://fengkehh.github.io/tags/tree/index.xml</link>
    <description>Recent content in Tree on A Song of Numbers and Graphs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://fengkehh.github.io/tags/tree/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Introduction to Cross Validation</title>
      <link>https://fengkehh.github.io/post/introduction-to-cross-validation/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://fengkehh.github.io/post/introduction-to-cross-validation/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basic-idea-keep-some-data-out-of-reach&#34;&gt;Basic Idea: Keep Some Data Out of Reach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cross-validation&#34;&gt;Cross Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#application-example&#34;&gt;Application Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;This is a continuation of my article on &lt;a href=&#34;https://fengkehh.github.io/post/2017-06-30-overfitting/&#34;&gt;overfitting&lt;/a&gt;. If you haven’t read it, I recommend you to start there first.&lt;/p&gt;
&lt;p&gt;As I mentioned, the biggest problem overfitting presents to a modeler is it causes us to think the model performance is better than it actually is. In this post I am going to introduce you to a resampling method that can produce accurate model performance estimation - &lt;strong&gt;cross validation&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;basic-idea-keep-some-data-out-of-reach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic Idea: Keep Some Data Out of Reach&lt;/h1&gt;
&lt;p&gt;So if you recall, &lt;a href=&#34;https://fengkehh.github.io/post/2017-06-30-overfitting/#a-simplified-simulation&#34;&gt;in the beginning of the simulation&lt;/a&gt; I splitted the data set into a training set and a validation set. The models were trained on the training set and they all produced inflated accuracies on those data points. However, when the models made predictions on the validation set their model accuracies all dropped significantly, particularly the ones that were overfitting. I was only able to get a sense of the real model performance by looking at the validation set accuracies and conclude that the simplest tree using the least amount of feature was in fact the best model.&lt;/p&gt;
&lt;p&gt;This is called the “hold-out” method. A portion of the data is held out of reach of the model training process. This portion of data is only used for the models to make predictions. The performance evaluation you get by letting the model make prediction on the held-out portion of the data is an estimation of the &lt;em&gt;out-of-sample&lt;/em&gt; performance. It’s out-of-sample because the model does not construct or tweak its structures/parameters using these datapoints. It’s an estimation because the portion of data held out is only a small subset of all the other possibilities that you would like to make a prediction on.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/training-validation.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;There is a bit of problem though.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The model you have is only built on the training data alone. The out-of-sample performance estimate you have is solely evaluated using the validation set. What if the data isn’t splitted perfectly? Let’s say we are looking at fitness data. By chance, most of the datapoints in the training set are from female participants and most of the datapoints in the validation set are from male participants. Can we still say that the model we built generalizes well to the entire population which has roughly equal amount of female and male? Can we say the performance estimation using a validation set containing only male is unbiased? The answer to both questions is of course, &lt;strong&gt;no&lt;/strong&gt;!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let’s say you measure your model performance on the validation set and want to tweak your model further. Like I mentioned &lt;a href=&#34;https://fengkehh.github.io/post/2017-06-30-overfitting/#discussion&#34;&gt;last time&lt;/a&gt;, doing so allows information from the validation set to leak out to the training process and effectively makes your model start training on the validation set. This can cause your model to overfit on the validation set and once again produce inflated performance estimation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let us address point 1 first.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cross Validation&lt;/h1&gt;
&lt;p&gt;The basic idea of cross validation is an extension of the hold out method.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Hold out a portion of the data to be the validation set. This is called a &lt;strong&gt;validation fold&lt;/strong&gt; or sometimes just a &lt;strong&gt;fold&lt;/strong&gt;. The rest of the data is set to be the training set. Train model on the training set and evaluate on the validation set/fold as usual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose a different portion of the data to be the validation set. To ensure no information is leaking over the new fold cannot overlap with the old fold (or any other folds for that matter). The rest is the training set. Train and evaluate.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Iterate steps 1 and 2 &lt;strong&gt;k&lt;/strong&gt; times. This ultimately results in k folds and k entries of performance evaluations. Their average is the out-of-sample performance estimate from &lt;strong&gt;k-fold&lt;/strong&gt; cross validation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here’s a picture visualizing a simple 2-fold cross validation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/2-fold_CV.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Notice that in the end ALL of the data points are used to train a model and ALL of them are also used for validation. The key is to recognize that&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Two models are built. Both with the same hyper-parameters but different training/validation sets.&lt;/li&gt;
&lt;li&gt;Whenever a datapoint is used to build a model, it is NOT used for validation of the said model. Vice versa.&lt;/li&gt;
&lt;li&gt;There is no overlap between the validation folds (the training sets can have overlaps although in our 2-fold case there’s no overlap there, either).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final performance estimate is an average of the model performance evaluations on the validation folds. This is an attempt to decrease the effect of hidden structures that aren’t perfectly balanced out from the data split.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Application Example&lt;/h1&gt;
&lt;p&gt;Let’s use the simulation data and the models we had last time to demonstrate cross-validation. Here’s a reminder for what the data represents &lt;a href=&#34;https://fengkehh.github.io/post/2017-06-30-overfitting/#a-simplified-simulation&#34;&gt;here&lt;/a&gt;. With cross-validation we actually don’t need to split the data into training and validation set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load all required libraries.
library(&amp;quot;caret&amp;quot;)
library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;rpart.plot&amp;quot;)

set.seed(123)
toss &amp;lt;- rbinom(1000, 1, 0.5)
inst &amp;lt;- rnorm(1000) + toss
volt &amp;lt;- rnorm(1000)
water &amp;lt;- rnorm(1000)

toss_fac &amp;lt;- factor(toss, labels = c(&amp;quot;tail&amp;quot;, &amp;quot;head&amp;quot;))

data &amp;lt;- data.frame(inst = inst, volt = volt, water = water, response = toss_fac)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall that &lt;code&gt;tree1&lt;/code&gt; overfits the data by being structurally complex. &lt;code&gt;tree2&lt;/code&gt; overfits by using too many features and &lt;code&gt;tree3&lt;/code&gt; tries to use a relatively simple structure and only the relevant feature. I will now estimate their model performances using 10-fold cross validation instead of the actual validation set. The code below generates a vector named &lt;code&gt;folds&lt;/code&gt; with the same size of the training data. The vector contains integers ranging from 1 to 10 randomly dispersed throughout with each integer occuring 100 times in total. What it represents is the validation fold the corresponding datapoint resides in. For example, if the 10th slot of the vector is 5, then the 10th datapoint is inside the 5th validation fold (and only the 5th).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_createFolds &amp;lt;- function(data, k, seed) {
    n &amp;lt;- nrow(data)
    folds &amp;lt;- rep(0, n)
    fold_size &amp;lt;- floor(n/k)
    # index of data points that haven&amp;#39;t been assigned a fold
    index_left &amp;lt;- 1:n
    
    for (i in 1:k) {
        if (i &amp;lt; k) {
            set.seed(seed)
            selected &amp;lt;- sample(1:length(index_left), fold_size)
            folds[index_left[selected]] &amp;lt;- i
            index_left &amp;lt;- index_left[-selected]
            
        } else {
            # Last fold. Assign everything left here.
            folds[index_left] &amp;lt;- k
        }
    }
    
    return(folds)
}

folds &amp;lt;- my_createFolds(data, 10, 123)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will now write a function to facillitate training the three types of tree model on each of the training set. Model performance is evaluated on each of the validation fold and the final performance is the average from all 10 folds. With three types of models I have to train a total of 3*10 = 30 models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function to train tree models and evaluate model performance using 10-fold
# CV

# Arguments: data: data to train the model on

# control: control objects that contains hyper-parameters for rpart.

# folds: cross validation folds. Length must be the number of rows in data.

# Returns: Model performance from each cross validation fold. Average
# returned vector to obtain final performance estimate!
tree_cv &amp;lt;- function(formula, data, control, folds) {
    k &amp;lt;- max(folds)
    n &amp;lt;- nrow(data)
    index &amp;lt;- 1:n
    accuracies &amp;lt;- rep(0, k)
    
    for (i in 1:k) {
        inFold &amp;lt;- index[folds == i]
        data.infold &amp;lt;- data[inFold, ]
        data.outside &amp;lt;- data[-inFold, ]
        
        # Train model on data outside of fold, predict on data in the fold, compute
        # accuracy.
        set.seed(1)
        model &amp;lt;- rpart(formula, data.outside, control = control)
        
        pred &amp;lt;- predict(model, newdata = data.infold, type = &amp;quot;class&amp;quot;)
        
        accuracies[i] &amp;lt;- sum(pred == data.infold[, all.vars(formula)[1]])/nrow(data.infold)
    }
    
    return(accuracies)
}

# Tree 1 Hyper-parameters
treeCon.over &amp;lt;- rpart.control(minsplit = 2, minbucket = 1, cp = 0, xval = 10)
# Tree 2 &amp;amp; 3 hyper-parameters
treeCon &amp;lt;- rpart.control(minsplit = 10, minbucket = 3, cp = 0.01, xval = 10)

tree1_accuracies &amp;lt;- tree_cv(response ~ inst, data = data, treeCon.over, folds)
tree2_accuracies &amp;lt;- tree_cv(response ~ inst + volt + water, data = data, treeCon, 
    folds)
tree3_accuracies &amp;lt;- tree_cv(response ~ inst, data = data, treeCon, folds)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a table showing the final estimated model accuracies from cross validations of the three models compared to their training set accuracies and validation set accuracies obtained last time:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Model.Type&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Training.Set.Accuracy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Validation.Set.Accuracy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;CV.Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Complex Structure Overfit&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6466&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.636&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Too Many Features&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7504&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6591&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.695&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Balanced&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7105&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6817&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.699&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, training set accuracies are higher for all three models compared to both validation set accuracies and CV estimated accuracies. This is particularly obvious for the first model. The CV estimated accuracies are in general much closer to the model performance on the validation set and they correctly reflect the true order of performance between the three models.&lt;/p&gt;
&lt;p&gt;You may have noticed there is a pattern for the gap between training set accuracies and their corresponding CV estimated accuracies: &lt;img src=&#34;../../post/2017-07-03-introduction-to-cross-validation_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yep, the higher amount of overfitting a model has the larger the gap is between its training set performance and CV estimated performance. This is perhaps not a surprise - during cross validation the model is always assessed using unseen data in the validation fold and the averaging step smoothes over the variation in model accuracy assessments. Thus the final assessment should be closer to the true performance and lower than the inflated performance a model has on its training set if it is overfitting. This can be used as a gauge to how much overfitting your model exhibits.&lt;/p&gt;
&lt;p&gt;This concludes the introduction to cross validation. There are more advanced things you can do with cross-validation, such as tuning model hyper-parameters and producing accurate performance assessments at the same time using nesting and subtleties like how to choose an effective k value that I did not cover. There are also other resampling methods for producing accurate performance estimates. Some are actually considered to be superior, such as the .632 bootstrap method. They shall be left for another time.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Demonstration of Overfitting</title>
      <link>https://fengkehh.github.io/post/2017-06-30-overfitting/</link>
      <pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://fengkehh.github.io/post/2017-06-30-overfitting/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-the-beginning-there-was-noise&#34;&gt;In the beginning, there was noise…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simplified-simulation&#34;&gt;A Simplified Simulation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-1-model-every-little-thing-from-the-instrument&#34;&gt;Model 1: Model every little thing from the instrument!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-2-include-all-features&#34;&gt;Model 2: Include all features!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-3-simple-structure-with-only-the-relevant-feature.&#34;&gt;Model 3: Simple structure with only the relevant feature.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance-evaluation-validation&#34;&gt;Performance Evaluation (Validation)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discussion&#34;&gt;Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;If you work with data and do any sort of model building, no doubt you have seen the word “overfitting” floating about. What is it and why do we need to care? Look on to find out!&lt;/p&gt;
&lt;div id=&#34;in-the-beginning-there-was-noise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In the beginning, there was noise…&lt;/h1&gt;
&lt;p&gt;Imagine you have a metal coin. The coin is coated with a very thin layer of rubber on only one side. You decide that you want to toss the coin in the air 1000 times and record the sound it makes when it lands on your table. It’s not a stretch to think it’s in fact possible to tell which side of the coin is ultimately facing up from the sound alone. The goal is predict which side is up using just the sound instead of Mark I Eyeballs.&lt;/p&gt;
&lt;p&gt;Unfortunately you have some important business to attend to so you ask your friend, Mad Scientist Mike, to conduct the experiment in your stead. When you come back, you find out that the microphone is also picking up Mike walking around in the room and the birds chirping outside. Furthermore, Mike has decided to be true to his name and recorded water pressure and voltage deviation of the house during the coin toss.&lt;/p&gt;
&lt;p&gt;You are now faced with a dilemma. Your gut feeling tells you some of the data you have may not be connected to your experiment outcomes. If that’s the case, then the fact that they are part of your measurements makes them unwanted &lt;strong&gt;noise&lt;/strong&gt;. If you are not careful when dealing with the noise, your model might become:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Overtly complicated, with parts and terms trying to explain the noise rather than actually important features.&lt;/li&gt;
&lt;li&gt;Poor in making predictions. Any slight fluctuations due to noise can cause the model to overreact!&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simplified-simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Simplified Simulation&lt;/h1&gt;
&lt;p&gt;Let’s use a simulated data set to help us understand what may happen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load all required libraries.
library(&amp;quot;caret&amp;quot;)
library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;rpart.plot&amp;quot;)

set.seed(123)
toss &amp;lt;- rbinom(1000, 1, 0.5)
inst &amp;lt;- rnorm(1000) + toss
volt &amp;lt;- rnorm(1000)
water &amp;lt;- rnorm(1000)

toss_fac &amp;lt;- factor(toss, labels = c(&amp;quot;tail&amp;quot;, &amp;quot;head&amp;quot;))

data &amp;lt;- data.frame(inst = inst, volt = volt, water = water, response = toss_fac)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response is 1000 tosses drawn from a binomial distribution and factored into either &lt;code&gt;tail&lt;/code&gt; or &lt;code&gt;head&lt;/code&gt;. The predictors are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;code&gt;inst&lt;/code&gt;rument measurement modeled simply as the response + a normally distributed signal noise.&lt;/li&gt;
&lt;li&gt;The deviation in &lt;code&gt;volt&lt;/code&gt;age in the electrical circuits of the house. Modeled as a standard normal distribution.&lt;/li&gt;
&lt;li&gt;The deviation in &lt;code&gt;water&lt;/code&gt; pressure of the house. Modeled as a standard normal distribution.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The data is then split into training and vadliation sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
inTrain &amp;lt;- createDataPartition(toss_fac, p = 0.6, list = FALSE)

data.train &amp;lt;- data[inTrain, ]
data.validation &amp;lt;- data[-inTrain, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am now going to train three classification tree models using the training sets, each demonstrating a different aspect.&lt;/p&gt;
&lt;div id=&#34;model-1-model-every-little-thing-from-the-instrument&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 1: Model every little thing from the instrument!&lt;/h2&gt;
&lt;p&gt;This model will use a huge amount of splitting and very &lt;em&gt;small&lt;/em&gt; leafs to fit the training data. The goal of this model is to try to achieve very high accuracy on the training set using the instrument measurement at all costs.&lt;/p&gt;
&lt;p&gt;First we will set up the hyper-parameters for the tree model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treeCon.over &amp;lt;- rpart.control(minsplit = 2, minbucket = 1, cp = 0, xval = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The exact meaning of the parameters are not super important but here it is:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;minsplit&lt;/code&gt;: controls how many training data points a node has to have before the algorithm can attempt to grow branches from it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;minbucket&lt;/code&gt;: how many training data points a leaf must at least have in the final tree.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp&lt;/code&gt;: controls how good a branch must be before it can be kept. Each branch must increase model quality (ie: R^2) by at least &lt;code&gt;cp&lt;/code&gt; or it will be cut-off.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;xval&lt;/code&gt;: number of cross-validation folds to be used for pruning (don’t worry if you don’t know what it means).&lt;/p&gt;
&lt;p&gt;So with this in mind, our tree is pretty much set up to be as complex as possible. Here’s the code to build the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree1 &amp;lt;- rpart(response ~ inst, data = data.train, control = treeCon.over)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that I have thrown away the other stuff Mike collected such as voltage and water pressure. The model is built using the noisy instrument signal as its sole predictor. Let us plot the tree structure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Whoa! That is indeed an extremely complex tree. However, what will the effect be on training set accuracy? Let’s find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train1 &amp;lt;- predict(tree1, data.train, type = &amp;quot;class&amp;quot;)
conMat1 &amp;lt;- confusionMatrix(pred.train1, data.train$response)
conMat1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  305    0
##       head    0  296
##                                     
##                Accuracy : 1         
##                  95% CI : (0.994, 1)
##     No Information Rate : 0.507     
##     P-Value [Acc &amp;gt; NIR] : &amp;lt;2e-16    
##                                     
##                   Kappa : 1         
##  Mcnemar&amp;#39;s Test P-Value : NA        
##                                     
##             Sensitivity : 1.000     
##             Specificity : 1.000     
##          Pos Pred Value : 1.000     
##          Neg Pred Value : 1.000     
##              Prevalence : 0.507     
##          Detection Rate : 0.507     
##    Detection Prevalence : 0.507     
##       Balanced Accuracy : 1.000     
##                                     
##        &amp;#39;Positive&amp;#39; Class : tail      
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An accuracy of 1. That is extremely high. Should we choose this one as our final model? Let’s build a couple other models first.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-include-all-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 2: Include all features!&lt;/h2&gt;
&lt;p&gt;Let’s change some hyper-parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treeCon &amp;lt;- rpart.control(minsplit = 10, minbucket = 3, cp = 0.01, xval = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have made each branching node and leaf larger, with also a higher quality improvement requirement during pruning. Let’s build the model now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree2 &amp;lt;- rpart(response ~ inst + volt + water, data = data.train, control = treeCon)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that although our hyper-parameters demand an overall decrease in structural complexity of the tree, we are now also using the other features Mike collected such as voltage and water pressure deviations to construct the model. Here’s the final strucutre:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, this is a much simpler tree in terms of sturctural complexity (albeit still pretty complex). However it does make use of all three features, some of them we suspect to be useless. What about its training set accuracy?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train2 &amp;lt;- predict(tree2, data.train, type = &amp;quot;class&amp;quot;)
conMat2 &amp;lt;- confusionMatrix(pred.train2, data.train$response)
conMat2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  213   58
##       head   92  238
##                                         
##                Accuracy : 0.75          
##                  95% CI : (0.714, 0.785)
##     No Information Rate : 0.507         
##     P-Value [Acc &amp;gt; NIR] : &amp;lt; 2e-16       
##                                         
##                   Kappa : 0.502         
##  Mcnemar&amp;#39;s Test P-Value : 0.00705       
##                                         
##             Sensitivity : 0.698         
##             Specificity : 0.804         
##          Pos Pred Value : 0.786         
##          Neg Pred Value : 0.721         
##              Prevalence : 0.507         
##          Detection Rate : 0.354         
##    Detection Prevalence : 0.451         
##       Balanced Accuracy : 0.751         
##                                         
##        &amp;#39;Positive&amp;#39; Class : tail          
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With an accuracy of 0.7504 it seems to be markedly worse than model 1…or is it? At the very least it still has quite a bit of predictive power since it’s accuracy is significantly higher than the &lt;em&gt;No Information Rate&lt;/em&gt;. Let’s build one final model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-simple-structure-with-only-the-relevant-feature.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 3: Simple structure with only the relevant feature.&lt;/h2&gt;
&lt;p&gt;No changes will be made to the hyper-parameters here. Instead, we will train the model with just one difference:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree3 &amp;lt;- rpart(response ~ inst, data = data.train, control = treeCon)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tree is built with the same complexity parameters as our last tree, but only using the instrument measurements as its feature. Let’s check out its structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly this is the simplest tree by far in terms of both structure and feature usage. How about its accuracy on the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train3 &amp;lt;- predict(tree3, data.train, type = &amp;quot;class&amp;quot;)
conMat3 &amp;lt;- confusionMatrix(pred.train3, data.train$response)
conMat3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  231  100
##       head   74  196
##                                         
##                Accuracy : 0.71          
##                  95% CI : (0.672, 0.746)
##     No Information Rate : 0.507         
##     P-Value [Acc &amp;gt; NIR] : &amp;lt;2e-16        
##                                         
##                   Kappa : 0.42          
##  Mcnemar&amp;#39;s Test P-Value : 0.0581        
##                                         
##             Sensitivity : 0.757         
##             Specificity : 0.662         
##          Pos Pred Value : 0.698         
##          Neg Pred Value : 0.726         
##              Prevalence : 0.507         
##          Detection Rate : 0.384         
##    Detection Prevalence : 0.551         
##       Balanced Accuracy : 0.710         
##                                         
##        &amp;#39;Positive&amp;#39; Class : tail          
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Uh oh. With an accuracy of 0.7105 this is by far the worst model we have built! I guess it’s clear we should use the big tree from model 1 as our final model, right? Oh wait, we split the data into training and validation set! Since the models are all built on the training set they have no ideas what the validation set looks like. If we feed the models the validation set we can make actual predictions with them and get a sense on its predictive accuracy. How exciting! Let’s do it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-evaluation-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance Evaluation (Validation)&lt;/h2&gt;
&lt;p&gt;Here’s the code to make predictions on the validation set and compute the prediction statistics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.validation.1 &amp;lt;- predict(tree1, data.validation, type = &amp;quot;class&amp;quot;)
pred.validation.2 &amp;lt;- predict(tree2, data.validation, type = &amp;quot;class&amp;quot;)
pred.validation.3 &amp;lt;- predict(tree3, data.validation, type = &amp;quot;class&amp;quot;)

confMat.valid.1 &amp;lt;- confusionMatrix(pred.validation.1, data.validation$response)
confMat.valid.2 &amp;lt;- confusionMatrix(pred.validation.2, data.validation$response)
confMat.valid.3 &amp;lt;- confusionMatrix(pred.validation.3, data.validation$response)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am just going to list the accuracies below in a table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(Model = c(&amp;quot;Model 1&amp;quot;, &amp;quot;Model 2&amp;quot;, &amp;quot;Model 3&amp;quot;), Accuracy = c(confMat.valid.1$overall[1], 
    confMat.valid.2$overall[1], confMat.valid.3$overall[1]))

kable(df)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6466&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6591&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6817&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Well well, the table has turned! The big tree in model1, although has an amazing training set accuracy, is now the definitive last place when it comes to making predictions on the validation set. The tree with the worst training set performance, the tree with low complexity parameters and only using the instrument measurement as its feature, is now the best performer. Why is that?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;By now you probably have a good idea to the answer already. The first two trees overfit the data (especially the first one). The first model contains too many nodes and leafs. The result is it is able to fit every single data point in the training set into single leafs by itself and simply look them up. No wonder it has such amazing (and grossly inflated) training set accuracy. However, when it comes to making actual predictions on the validation set all the noise throws it off and causes it to overreact on noisy signals that don’t have any real effect on the response. The result is terrible predictive accuracy.&lt;/p&gt;
&lt;p&gt;Although the second tree tries to avoid very complex structure, it uses features that are clearly useless in predicting the response (water pressure and voltage deviations…really, Mike?). The good thing is the pruning process is able to catch some of these and cut them off the trees so it doesn’t make too much of a negative impact on its prediction accuracy.&lt;/p&gt;
&lt;p&gt;Nevertheless, you can see that model 3, with the simplest structure and using only the relevant feature, turns out to be the best one in the end. This simple structure is achieved by &lt;em&gt;tuning hyper-parameters&lt;/em&gt; and using only relevant features aka &lt;em&gt;feature selection&lt;/em&gt;. In this case, they are largely done by experience and logic. I know beforehand that the data is noisy and two of the features are useless, so all I need to do is just selecting sensible model parameter values relative to my other choices and get rid of useless features. This is actually an &lt;em&gt;excellent&lt;/em&gt; way to build models as it is not done using information obtained directly from the data so there is no risk of overfitting.&lt;/p&gt;
&lt;p&gt;For a model builder, the biggest problem with overfitting is it contaminates model performance assessment which leads to making the wrong choice. Think back on the inflated training set accuracies. If I used the training set accuracy as the performance indicator I would have chosen the worst model in the end. Some may ask, what if I tweak model parameters and select features using the statistics produced by a previous model? The truth is, &lt;strong&gt;whenever you are using the data to gain information about what your model should be, what features you shoud select etc, you can potentially overfit&lt;/strong&gt;. For example, now I know the prediction accuracies of the three models on the validation set, if I go back, tweak some parameters/select some features and retrain the model on the training set to improve my prediction accuracy on the validation set, the validation set accuracy is likely going to be biased due to overfitting as well. Why? Because the accuracy assessment on the validation set is information gleaned from the validation set. By going back to make model tweaks based on it, information in the validation set is now spilling over to the model training process and the validation set accuracy will likely become biased just like the training set accuracies.&lt;/p&gt;
&lt;p&gt;Wait, does this mean we can only use the validation set to produce an accurate estimate of model performance ONCE? Well, technically, yes. However, there are techniques to repeatedly produce accurate performance estimates and avoid/delay bias caused by overfitting such as &lt;strong&gt;resampling&lt;/strong&gt;. We shall see this in another post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>