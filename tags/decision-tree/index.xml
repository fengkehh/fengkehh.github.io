<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Decision Tree on A Song of Numbers and Graphs</title>
    <link>https://fengkehh.github.io/tags/decision-tree/index.xml</link>
    <description>Recent content in Decision Tree on A Song of Numbers and Graphs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://fengkehh.github.io/tags/decision-tree/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Demonstration of Overfitting</title>
      <link>https://fengkehh.github.io/post/2017-06-30-overfitting/</link>
      <pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://fengkehh.github.io/post/2017-06-30-overfitting/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-the-beginning-there-was-noise&#34;&gt;In the beginning, there was noise…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simplified-simulation&#34;&gt;A Simplified Simulation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-1-model-every-little-thing-from-the-instrument&#34;&gt;Model 1: Model every little thing from the instrument!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-2-include-all-features&#34;&gt;Model 2: Include all features!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-3-simple-structure-with-only-the-relevant-feature.&#34;&gt;Model 3: Simple structure with only the relevant feature.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance-evaluation-validation&#34;&gt;Performance Evaluation (Validation)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discussion&#34;&gt;Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;If you work with data and do any sort of model building, no doubt you have seen the word “overfitting” floating about. What is it and why do we need to care? Look on to find out!&lt;/p&gt;
&lt;div id=&#34;in-the-beginning-there-was-noise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In the beginning, there was noise…&lt;/h1&gt;
&lt;p&gt;Imagine you have a metal coin. The coin is coated with a very thin layer of rubber on only one side. You decide that you want to toss the coin in the air 1000 times and record the sound it makes when it lands on your table. It’s not a stretch to think it’s in fact possible to tell which side of the coin is ultimately facing up from the sound alone. The goal is predict which side is up using just the sound instead of Mark I Eyeballs.&lt;/p&gt;
&lt;p&gt;Unfortunately you have some important business to attend to so you ask your friend, Mad Scientist Mike, to conduct the experiment in your stead. When you come back, you find out that the microphone is also picking up Mike walking around in the room and the birds chirping outside. Furthermore, Mike has decided to be true to his name and recorded water pressure and voltage deviation of the house in the mean time.&lt;/p&gt;
&lt;p&gt;You are now faced with a dilemma. Your gut feeling tells you some of the data you have may not be connected to your experiment outcomes. If that’s the case, then they are part of your measurements makes them unwanted &lt;strong&gt;noise&lt;/strong&gt;. If you are not careful when dealing with the noise, your model might become:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Overtly complicated, with parts and terms trying to explain the noise rather than actually important features.&lt;/li&gt;
&lt;li&gt;Poor in making predictions. Any slight fluctuations due to noise can cause the model to overreact!&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simplified-simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Simplified Simulation&lt;/h1&gt;
&lt;p&gt;Let’s use a simulated data set to help us understand what may happen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load all required libraries.
library(&amp;quot;caret&amp;quot;)
library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;rpart.plot&amp;quot;)

set.seed(123)
toss &amp;lt;- rbinom(1000, 1, 0.5)
inst &amp;lt;- rnorm(1000) + toss
volt &amp;lt;- rnorm(1000)
water &amp;lt;- rnorm(1000)

toss_fac &amp;lt;- factor(toss, labels = c(&amp;quot;tail&amp;quot;, &amp;quot;head&amp;quot;))

data &amp;lt;- data.frame(inst = inst, volt = volt, water = water, response = toss_fac)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response is 1000 tosses drawn from a binomial distribution and factored into either &lt;code&gt;tail&lt;/code&gt; or &lt;code&gt;head&lt;/code&gt;. The predictors are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;code&gt;inst&lt;/code&gt;rument measurement modeled simply as the response + a normally distributed signal noise.&lt;/li&gt;
&lt;li&gt;The deviation in &lt;code&gt;volt&lt;/code&gt;age in the electrical circuits of the house. Modeled as a standard normal distribution.&lt;/li&gt;
&lt;li&gt;The deviation in &lt;code&gt;water&lt;/code&gt; pressure of the house. Modeled as a standard normal distribution.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The data is then split into training and vadliation sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inTrain &amp;lt;- createDataPartition(toss_fac, p = 0.6, list = FALSE)

data.train &amp;lt;- data[inTrain, ]
data.validation &amp;lt;- data[-inTrain, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am now going to train three decision tree models using the training sets, each demonstrating a different aspect.&lt;/p&gt;
&lt;div id=&#34;model-1-model-every-little-thing-from-the-instrument&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 1: Model every little thing from the instrument!&lt;/h2&gt;
&lt;p&gt;This model will use a huge amount of splitting and very &lt;em&gt;small&lt;/em&gt; leafs to fit the training data. The goal of this model is to try to achieve very high accuracy on the training set using the instrument measurement at all costs.&lt;/p&gt;
&lt;p&gt;First we will set up the hyper-parameters for the tree model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treeCon.over &amp;lt;- rpart.control(minsplit = 2, minbucket = 1, cp = 0, xval = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The exact meaning of the parameters are not super important but here it is:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;minsplit&lt;/code&gt;: controls how many training data points a node has to have before the algorithm can attempt to grow branches from it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;minbucket&lt;/code&gt;: how many training data points a leaf must at least have in the final tree.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp&lt;/code&gt;: controls how good a branch must be before it can be kept. Each branch must increase model quality (ie: R^2) by at least &lt;code&gt;cp&lt;/code&gt; or it will be cut-off.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;xval&lt;/code&gt;: number of cross-validation folds to be used for pruning (don’t worry if you don’t know what it means).&lt;/p&gt;
&lt;p&gt;So with this in mind, our tree is pretty much set up to be as complex as possible. Here’s the code to build the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree1 &amp;lt;- rpart(response ~ inst, data = data.train, control = treeCon.over)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that I have thrown away the other stuff Mike collected such as voltage and water pressure. The model is built using the noisy instrument signal as its sole predictor. Let us plot the tree structure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Whoa! That is indeed an extremely complex tree. However, what will the effect be on training set accuracy? Let’s find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train1 &amp;lt;- predict(tree1, data.train, type = &amp;quot;class&amp;quot;)
conMat1 &amp;lt;- confusionMatrix(pred.train1, data.train$response)
conMat1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  305    3
##       head    0  293
##                                          
##                Accuracy : 0.995          
##                  95% CI : (0.9855, 0.999)
##     No Information Rate : 0.5075         
##     P-Value [Acc &amp;gt; NIR] : &amp;lt;2e-16         
##                                          
##                   Kappa : 0.99           
##  Mcnemar&amp;#39;s Test P-Value : 0.2482         
##                                          
##             Sensitivity : 1.0000         
##             Specificity : 0.9899         
##          Pos Pred Value : 0.9903         
##          Neg Pred Value : 1.0000         
##              Prevalence : 0.5075         
##          Detection Rate : 0.5075         
##    Detection Prevalence : 0.5125         
##       Balanced Accuracy : 0.9949         
##                                          
##        &amp;#39;Positive&amp;#39; Class : tail           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An accuracy of 0.9950083. That is extremely high. Should we choose this one as our final model? Let’s build a couple other models first.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-include-all-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 2: Include all features!&lt;/h2&gt;
&lt;p&gt;Let’s change some hyper-parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treeCon &amp;lt;- rpart.control(minsplit = 10, minbucket = 3, cp = 0.01, xval = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have made each branching node and leaf larger, with also a higher quality improvement requirement during pruning. Let’s build the model now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree2 &amp;lt;- rpart(response ~ inst + volt + water, data = data.train, control = treeCon)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that although our hyper-parameters demand an overall decrease in structural complexity of the tree, we are now also using the other features Mike collected such as voltage and water pressure deviations to construct the model. Here’s the final strucutre:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, this is a much simpler tree in terms of sturctural complexity (albeit still pretty complex). However it does make use of all three features, some of them we suspect to be useless. What about its training set accuracy?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train2 &amp;lt;- predict(tree2, data.train, type = &amp;quot;class&amp;quot;)
conMat2 &amp;lt;- confusionMatrix(pred.train2, data.train$response)
conMat2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  248   76
##       head   57  220
##                                           
##                Accuracy : 0.7787          
##                  95% CI : (0.7434, 0.8113)
##     No Information Rate : 0.5075          
##     P-Value [Acc &amp;gt; NIR] : &amp;lt;2e-16          
##                                           
##                   Kappa : 0.5569          
##  Mcnemar&amp;#39;s Test P-Value : 0.1186          
##                                           
##             Sensitivity : 0.8131          
##             Specificity : 0.7432          
##          Pos Pred Value : 0.7654          
##          Neg Pred Value : 0.7942          
##              Prevalence : 0.5075          
##          Detection Rate : 0.4126          
##    Detection Prevalence : 0.5391          
##       Balanced Accuracy : 0.7782          
##                                           
##        &amp;#39;Positive&amp;#39; Class : tail            
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With an accuracy of 0.7787022 it seems to be markedly worse than model 1…or is it? At the very least it still has quite a bit of predictive power since it’s accuracy is significantly higher than the &lt;em&gt;No Information Rate&lt;/em&gt;. Let’s build one final model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-simple-structure-with-only-the-relevant-feature.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 3: Simple structure with only the relevant feature.&lt;/h2&gt;
&lt;p&gt;No changes will be made to the hyper-parameters here. Instead, we will train the model with just one difference:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree3 &amp;lt;- rpart(response ~ inst, data = data.train, control = treeCon)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tree is built with the same complexity parameters as our last tree, but only using the instrument measurements as its feature. Let’s check out its structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly this is the simplest tree by far in terms of both structure and feature usage. How about its accuracy on the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train3 &amp;lt;- predict(tree3, data.train, type = &amp;quot;class&amp;quot;)
conMat3 &amp;lt;- confusionMatrix(pred.train3, data.train$response)
conMat3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  262  123
##       head   43  173
##                                           
##                Accuracy : 0.7238          
##                  95% CI : (0.6862, 0.7592)
##     No Information Rate : 0.5075          
##     P-Value [Acc &amp;gt; NIR] : &amp;lt; 2.2e-16       
##                                           
##                   Kappa : 0.4453          
##  Mcnemar&amp;#39;s Test P-Value : 8.701e-10       
##                                           
##             Sensitivity : 0.8590          
##             Specificity : 0.5845          
##          Pos Pred Value : 0.6805          
##          Neg Pred Value : 0.8009          
##              Prevalence : 0.5075          
##          Detection Rate : 0.4359          
##    Detection Prevalence : 0.6406          
##       Balanced Accuracy : 0.7217          
##                                           
##        &amp;#39;Positive&amp;#39; Class : tail            
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Uh oh. With an accuracy of 0.7237937 this is by far the worst model we have built! I guess it’s clear we should use the big tree from model 1 as our final model, right? Oh wait, we split the data into training and validation set! Since the models are all built on the training set they have no ideas what the validation set looks like. If we feed the models the validation set we can make actual predictions with them and get a sense on its predictive accuracy. How exciting! Let’s do it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-evaluation-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance Evaluation (Validation)&lt;/h2&gt;
&lt;p&gt;Here’s the code to make predictions on the validation set and compute the prediction statistics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.validation.1 &amp;lt;- predict(tree1, data.validation, type = &amp;quot;class&amp;quot;)
pred.validation.2 &amp;lt;- predict(tree2, data.validation, type = &amp;quot;class&amp;quot;)
pred.validation.3 &amp;lt;- predict(tree3, data.validation, type = &amp;quot;class&amp;quot;)

confMat.valid.1 &amp;lt;- confusionMatrix(pred.validation.1, data.validation$response)
confMat.valid.2 &amp;lt;- confusionMatrix(pred.validation.2, data.validation$response)
confMat.valid.3 &amp;lt;- confusionMatrix(pred.validation.3, data.validation$response)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am just going to list the accuracies below in a table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(Model = c(&amp;quot;Model 1&amp;quot;, &amp;quot;Model 2&amp;quot;, &amp;quot;Model 3&amp;quot;), Accuracy = c(confMat.valid.1$overall[1], 
    confMat.valid.2$overall[1], confMat.valid.3$overall[1]))

kable(df)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6065163&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6691729&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6791980&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Well well, the table has turned! The big tree in model1, although has an amazing training set accuracy, is now the definitive last place when it comes to making predictions on the validation set. The tree with the worst training set performance, the tree with low complexity parameters and only using the instrument measurement as its feature, is now the best performer. Why is that?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;By now you probably have a good idea to the answer already. The first two trees overfit the data (especially the first one). The first model contains too many nodes and leafs. The result is it is able to fit every single data point in the training set into single leafs by itself and simply look them up. No wonder it has such amazing (and grossly inflated) training set accuracy. However, when it comes to making actual predictions on the validation set all the noise throws it off and causes it to overreact on noisy signals that don’t have any real effect on the response. The result is terrible predictive accuracy.&lt;/p&gt;
&lt;p&gt;Although the second tree tries to avoid very complex structure, it uses features that are clearly useless in predicting the response (water pressure and voltage deviations…really, Mike?). The good thing is the pruning process is able to catch some of these and cut them off the trees so it doesn’t make too much of a negative impact on its prediction accuracy.&lt;/p&gt;
&lt;p&gt;Nevertheless, you can see that model 3, with the simplest structure and using only the relevant feature, turns out to be the best one in the end. For a model builder, the biggest problem with overfitting is it contaminates the performance assessment and can cause us to make the wrong choice. Think back on the inflated training set accuracies. If I chose the final based on that I would have chosen the worst model in the end. The simple structure in model 3 is achieved by &lt;em&gt;tuning hyper-parameters&lt;/em&gt; and using only relevant features aka &lt;em&gt;feature selection&lt;/em&gt;. In this case, these steps are largely done by experience and logic. I know beforehand that the data is noisy and two of the features are useless, so all I need to do is just selecting sensible model parameter values relative to my other choices and get rid of useless features. This is actually an &lt;em&gt;excellent&lt;/em&gt; way to build models as it is not done using information obtained directly from the data so there is no risk of overfitting.&lt;/p&gt;
&lt;p&gt;Some may ask, what if I tweak model parameters and select features using the statistics produced by a previous model? The truth is, &lt;strong&gt;whenever you are using the data to gain information about what your model should be, what features you shoud select etc, you can potentially overfit&lt;/strong&gt;. For example, now I know the prediction accuracies of the three models on the validation set, if I go back, tweak some parameters/select some features and retrain the model on the training set to improve my prediction accuracy on the validation set, the validation set accuracy is likely going to be biased due to overfitting as well. Why? Because the accuracy assessment on the validation set is information gleaned from the validation set. By going back to make model tweaks based on it, information in the validation set is now spilling over to the model training process and the validation set accuracy will likely become biased just like the training set accuracies.&lt;/p&gt;
&lt;p&gt;Wait, does this mean we can only use the validation set to produce an accurate estimate of model performance ONCE? Well, technically, yes. However, there are techniques to repeatedly produce accurate performance estimates and avoid/delay bias caused by overfitting such as &lt;strong&gt;resampling&lt;/strong&gt;. We shall see this in another post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>