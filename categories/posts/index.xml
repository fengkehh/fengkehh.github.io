<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on A Song of Numbers and Graphs</title>
    <link>https://fengkehh.github.io/categories/posts/index.xml</link>
    <description>Recent content in Posts on A Song of Numbers and Graphs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://fengkehh.github.io/categories/posts/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Introduction to Cross Validation</title>
      <link>https://fengkehh.github.io/post/introduction-to-cross-validation/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://fengkehh.github.io/post/introduction-to-cross-validation/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basic-idea-keep-some-data-out-of-reach&#34;&gt;Basic Idea: Keep Some Data Out of Reach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cross-validation&#34;&gt;Cross Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#application-example&#34;&gt;Application Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;This is a continuation of my article on &lt;a href=&#34;https://fengkehh.github.io/post/2017-06-30-overfitting/&#34;&gt;overfitting&lt;/a&gt;. If you haven’t read it, I recommend you to start there first.&lt;/p&gt;
&lt;p&gt;As I mentioned, the biggest problem overfitting presents to a modeler is it causes us to think the model performance is better than it actually is. In this post I am going to introduce you to a resampling method that can produce accurate model performance estimation - &lt;strong&gt;cross validation&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;basic-idea-keep-some-data-out-of-reach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic Idea: Keep Some Data Out of Reach&lt;/h1&gt;
&lt;p&gt;So if you recall, &lt;a href=&#34;https://fengkehh.github.io/post/2017-06-30-overfitting/#a-simplified-simulation&#34;&gt;in the beginning of the simulation&lt;/a&gt; I splitted the data set into a training set and a validation set. The models were trained on the training set and they all produced inflated accuracies on those data points. However, when the models made predictions on the validation set their model accuracies all dropped significantly, particularly the ones that were overfitting. I was only able to get a sense of the real model performance by looking at the validation set accuracies and conclude that the simplest tree using the least amount of feature was in fact the best model.&lt;/p&gt;
&lt;p&gt;This is called the “hold-out” method. A portion of the data is held out of reach of the model training process. This portion of data is only used for the models to make predictions. The performance evaluation you get by letting the model make prediction on the held-out portion of the data is an estimation of the &lt;em&gt;out-of-sample&lt;/em&gt; performance. It’s out-of-sample because the model does not construct or tweak its structures/parameters using these datapoints. It’s an estimation because the portion of data held out is only a small subset of all the other possibilities that you would like to make a prediction on.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/training-validation.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;There is a bit of problem though.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The model you have is only built on the training data alone. The out-of-sample performance estimate you have is solely evaluated using the validation set. What if the data isn’t splitted perfectly? Let’s say we are looking at fitness data. By chance, most of the datapoints in the training set are from female participants and most of the datapoints in the validation set are from male participants. Can we still say that the model we built generalizes well to the entire population which has roughly equal amount of female and male? Can we say the performance estimation using a validation set containing only male is unbiased? The answer to both questions is of course, &lt;strong&gt;no&lt;/strong&gt;!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let’s say you measure your model performance on the validation set and want to tweak your model further. Like I mentioned &lt;a href=&#34;https://fengkehh.github.io/post/2017-06-30-overfitting/#discussion&#34;&gt;last time&lt;/a&gt;, doing so allows information from the validation set to leak out to the training process and effectively makes your model start training on the validation set. This can cause your model to overfit on the validation set and once again produce inflated performance estimation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let us address point 1 first.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cross Validation&lt;/h1&gt;
&lt;p&gt;The basic idea of cross validation is an extension of the hold out method.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Hold out a portion of the data to be the validation set. This is called a &lt;strong&gt;validation fold&lt;/strong&gt; or sometimes just a &lt;strong&gt;fold&lt;/strong&gt;. The rest of the data is set to be the training set. Train model on the training set and evaluate on the validation set/fold as usual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose a different portion of the data to be the validation set. To ensure no information is leaking over the new fold cannot overlap with the old fold (or any other folds for that matter). The rest is the training set. Train and evaluate.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Iterate steps 1 and 2 &lt;strong&gt;k&lt;/strong&gt; times. This ultimately results in k folds and k entries of performance evaluations. Their average is the out-of-sample performance estimate from &lt;strong&gt;k-fold&lt;/strong&gt; cross validation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here’s a picture visualizing a simple 2-fold cross validation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/2-fold_CV.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Notice that in the end ALL of the data points are used to train a model and ALL of them are also used for validation. The key is to recognize that&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Two models are built. Both with the same hyper-parameters but different training/validation sets.&lt;/li&gt;
&lt;li&gt;Whenever a datapoint is used to build a model, it is NOT used for validation of the said model. Vice versa.&lt;/li&gt;
&lt;li&gt;There is no overlap between the validation folds (the training sets can have overlaps although in our 2-fold case there’s no overlap there, either).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final performance estimate is an average of the model performance evaluations on the validation folds. This is an attempt to decrease the effect of hidden structures that aren’t perfectly balanced out from the data split.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Application Example&lt;/h1&gt;
&lt;p&gt;Let’s use the simulation data and the models we had last time to demonstrate cross-validation. Here’s a reminder for what the data represents &lt;a href=&#34;https://fengkehh.github.io/post/2017-06-30-overfitting/#a-simplified-simulation&#34;&gt;here&lt;/a&gt;. With cross-validation we actually don’t need to split the data into training and validation set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load all required libraries.
library(&amp;quot;caret&amp;quot;)
library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;rpart.plot&amp;quot;)

set.seed(123)
toss &amp;lt;- rbinom(1000, 1, 0.5)
inst &amp;lt;- rnorm(1000) + toss
volt &amp;lt;- rnorm(1000)
water &amp;lt;- rnorm(1000)

toss_fac &amp;lt;- factor(toss, labels = c(&amp;quot;tail&amp;quot;, &amp;quot;head&amp;quot;))

data &amp;lt;- data.frame(inst = inst, volt = volt, water = water, response = toss_fac)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall that &lt;code&gt;tree1&lt;/code&gt; overfits the data by being structurally complex. &lt;code&gt;tree2&lt;/code&gt; overfits by using too many features and &lt;code&gt;tree3&lt;/code&gt; tries to use a relatively simple structure and only the relevant feature. I will now estimate their model performances using 10-fold cross validation instead of the actual validation set. The code below generates a vector named &lt;code&gt;folds&lt;/code&gt; with the same size of the training data. The vector contains integers ranging from 1 to 10 randomly dispersed throughout with each integer occuring 100 times in total. What it represents is the validation fold the corresponding datapoint resides in. For example, if the 10th slot of the vector is 5, then the 10th datapoint is inside the 5th validation fold (and only the 5th).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;folds &amp;lt;- rep(0, nrow(data))
k &amp;lt;- 10
n &amp;lt;- nrow(data)
fold_size &amp;lt;- floor(n/k)
# index of data points that haven&amp;#39;t been assigned a fold
index_left &amp;lt;- 1:n
# Random number seed for reproducibility
seed = 123

for (i in 1:k) {
    if (i &amp;lt; k) {
        set.seed(seed)
        selected &amp;lt;- sample(1:length(index_left), fold_size)
        folds[index_left[selected]] &amp;lt;- i
        index_left &amp;lt;- index_left[-selected]
        
    } else {
        # Last fold. Assign everything left here.
        folds[index_left] &amp;lt;- k
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will now write a function to facillitate training the three types of tree model on each of the training set. Model performance is evaluated on each of the validation fold and the final performance is the average from all 10 folds. With three types of models I have to train a total of 3*10 = 30 models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function to train tree models and evaluate model performance using 10-fold
# CV

# Arguments: data: data to train the model on

# control: control objects that contains hyper-parameters for rpart.

# folds: cross validation folds. Length must be the number of rows in data.

# Returns: Model performance from each cross validation fold. Average
# returned vector to obtain final performance estimate!
tree_cv &amp;lt;- function(formula, data, control, folds) {
    k &amp;lt;- max(folds)
    n &amp;lt;- nrow(data)
    index &amp;lt;- 1:n
    accuracies &amp;lt;- rep(0, k)
    
    for (i in 1:k) {
        inFold &amp;lt;- index[folds == i]
        data.infold &amp;lt;- data[inFold, ]
        data.outside &amp;lt;- data[-inFold, ]
        
        # Train model on data outside of fold, predict on data in the fold, compute
        # accuracy.
        set.seed(1)
        model &amp;lt;- rpart(formula, data.outside, control = control)
        
        pred &amp;lt;- predict(model, newdata = data.infold, type = &amp;quot;class&amp;quot;)
        
        accuracies[i] &amp;lt;- sum(pred == data.infold[, all.vars(formula)[1]])/nrow(data.infold)
    }
    
    return(accuracies)
}

# Tree 1 Hyper-parameters
treeCon.over &amp;lt;- rpart.control(minsplit = 2, minbucket = 1, cp = 0, xval = 10)
# Tree 2 &amp;amp; 3 hyper-parameters
treeCon &amp;lt;- rpart.control(minsplit = 10, minbucket = 3, cp = 0.01, xval = 10)

tree1_accuracies &amp;lt;- tree_cv(response ~ inst, data = data, treeCon.over, folds)
tree2_accuracies &amp;lt;- tree_cv(response ~ inst + volt + water, data = data, treeCon, 
    folds)
tree3_accuracies &amp;lt;- tree_cv(response ~ inst, data = data, treeCon, folds)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a table showing the final estimated model accuracies from cross validations of the three models compared to their training set accuracies and validation set accuracies obtained last time:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Model.Type&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Training.Set.Accuracy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Validation.Set.Accuracy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;CV.Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Complex Structure Overfit&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6466&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.636&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Too Many Features&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7504&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6591&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.695&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Balanced&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7105&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6817&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.699&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, training set accuracies are higher for all three models compared to both validation set accuracies and CV estimated accuracies. This is particularly obvious for the first model. The CV estimated accuracies are in general much closer to the model performance on the validation set and they correctly reflect the true order of performance between the three models.&lt;/p&gt;
&lt;p&gt;You may have noticed there is a pattern for the gap between training set accuracies and their corresponding CV estimated accuracies: &lt;img src=&#34;../../post/2017-07-03-introduction-to-cross-validation_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yep, the higher amount of overfitting a model has the larger the gap is between its training set performance and CV estimated performance. This is perhaps not a surprise - during cross validation the model is always assessed using unseen data in the validation fold and the averaging step smoothes over the variation in model accuracy assessments. Thus the final assessment should be closer to the true performance and lower than the inflated performance a model has on its training set if it is overfitting. This can be used as a gauge to how much overfitting your model exhibits.&lt;/p&gt;
&lt;p&gt;This concludes the introduction to cross validation. There are more advanced things you can do with cross-validation, such as tuning model hyper-parameters and producing accurate performance assessments at the same time using nesting and subtleties like how to choose an effective k value that I did not cover. There are also other resampling methods for producing accurate performance estimates. Some are actually considered to be superior, such as the .632 bootstrap method. They shall be left for another time.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Demonstration of Overfitting</title>
      <link>https://fengkehh.github.io/post/2017-06-30-overfitting/</link>
      <pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://fengkehh.github.io/post/2017-06-30-overfitting/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-the-beginning-there-was-noise&#34;&gt;In the beginning, there was noise…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simplified-simulation&#34;&gt;A Simplified Simulation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-1-model-every-little-thing-from-the-instrument&#34;&gt;Model 1: Model every little thing from the instrument!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-2-include-all-features&#34;&gt;Model 2: Include all features!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-3-simple-structure-with-only-the-relevant-feature.&#34;&gt;Model 3: Simple structure with only the relevant feature.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance-evaluation-validation&#34;&gt;Performance Evaluation (Validation)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discussion&#34;&gt;Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;If you work with data and do any sort of model building, no doubt you have seen the word “overfitting” floating about. What is it and why do we need to care? Look on to find out!&lt;/p&gt;
&lt;div id=&#34;in-the-beginning-there-was-noise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In the beginning, there was noise…&lt;/h1&gt;
&lt;p&gt;Imagine you have a metal coin. The coin is coated with a very thin layer of rubber on only one side. You decide that you want to toss the coin in the air 1000 times and record the sound it makes when it lands on your table. It’s not a stretch to think it’s in fact possible to tell which side of the coin is ultimately facing up from the sound alone. The goal is predict which side is up using just the sound instead of Mark I Eyeballs.&lt;/p&gt;
&lt;p&gt;Unfortunately you have some important business to attend to so you ask your friend, Mad Scientist Mike, to conduct the experiment in your stead. When you come back, you find out that the microphone is also picking up Mike walking around in the room and the birds chirping outside. Furthermore, Mike has decided to be true to his name and recorded water pressure and voltage deviation of the house during the coin toss.&lt;/p&gt;
&lt;p&gt;You are now faced with a dilemma. Your gut feeling tells you some of the data you have may not be connected to your experiment outcomes. If that’s the case, then the fact that they are part of your measurements makes them unwanted &lt;strong&gt;noise&lt;/strong&gt;. If you are not careful when dealing with the noise, your model might become:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Overtly complicated, with parts and terms trying to explain the noise rather than actually important features.&lt;/li&gt;
&lt;li&gt;Poor in making predictions. Any slight fluctuations due to noise can cause the model to overreact!&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simplified-simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Simplified Simulation&lt;/h1&gt;
&lt;p&gt;Let’s use a simulated data set to help us understand what may happen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load all required libraries.
library(&amp;quot;caret&amp;quot;)
library(&amp;quot;rpart&amp;quot;)
library(&amp;quot;rpart.plot&amp;quot;)

set.seed(123)
toss &amp;lt;- rbinom(1000, 1, 0.5)
inst &amp;lt;- rnorm(1000) + toss
volt &amp;lt;- rnorm(1000)
water &amp;lt;- rnorm(1000)

toss_fac &amp;lt;- factor(toss, labels = c(&amp;quot;tail&amp;quot;, &amp;quot;head&amp;quot;))

data &amp;lt;- data.frame(inst = inst, volt = volt, water = water, response = toss_fac)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response is 1000 tosses drawn from a binomial distribution and factored into either &lt;code&gt;tail&lt;/code&gt; or &lt;code&gt;head&lt;/code&gt;. The predictors are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;code&gt;inst&lt;/code&gt;rument measurement modeled simply as the response + a normally distributed signal noise.&lt;/li&gt;
&lt;li&gt;The deviation in &lt;code&gt;volt&lt;/code&gt;age in the electrical circuits of the house. Modeled as a standard normal distribution.&lt;/li&gt;
&lt;li&gt;The deviation in &lt;code&gt;water&lt;/code&gt; pressure of the house. Modeled as a standard normal distribution.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The data is then split into training and vadliation sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
inTrain &amp;lt;- createDataPartition(toss_fac, p = 0.6, list = FALSE)

data.train &amp;lt;- data[inTrain, ]
data.validation &amp;lt;- data[-inTrain, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am now going to train three classification tree models using the training sets, each demonstrating a different aspect.&lt;/p&gt;
&lt;div id=&#34;model-1-model-every-little-thing-from-the-instrument&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 1: Model every little thing from the instrument!&lt;/h2&gt;
&lt;p&gt;This model will use a huge amount of splitting and very &lt;em&gt;small&lt;/em&gt; leafs to fit the training data. The goal of this model is to try to achieve very high accuracy on the training set using the instrument measurement at all costs.&lt;/p&gt;
&lt;p&gt;First we will set up the hyper-parameters for the tree model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treeCon.over &amp;lt;- rpart.control(minsplit = 2, minbucket = 1, cp = 0, xval = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The exact meaning of the parameters are not super important but here it is:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;minsplit&lt;/code&gt;: controls how many training data points a node has to have before the algorithm can attempt to grow branches from it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;minbucket&lt;/code&gt;: how many training data points a leaf must at least have in the final tree.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp&lt;/code&gt;: controls how good a branch must be before it can be kept. Each branch must increase model quality (ie: R^2) by at least &lt;code&gt;cp&lt;/code&gt; or it will be cut-off.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;xval&lt;/code&gt;: number of cross-validation folds to be used for pruning (don’t worry if you don’t know what it means).&lt;/p&gt;
&lt;p&gt;So with this in mind, our tree is pretty much set up to be as complex as possible. Here’s the code to build the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree1 &amp;lt;- rpart(response ~ inst, data = data.train, control = treeCon.over)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that I have thrown away the other stuff Mike collected such as voltage and water pressure. The model is built using the noisy instrument signal as its sole predictor. Let us plot the tree structure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Whoa! That is indeed an extremely complex tree. However, what will the effect be on training set accuracy? Let’s find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train1 &amp;lt;- predict(tree1, data.train, type = &amp;quot;class&amp;quot;)
conMat1 &amp;lt;- confusionMatrix(pred.train1, data.train$response)
conMat1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  305    0
##       head    0  296
##                                     
##                Accuracy : 1         
##                  95% CI : (0.994, 1)
##     No Information Rate : 0.507     
##     P-Value [Acc &amp;gt; NIR] : &amp;lt;2e-16    
##                                     
##                   Kappa : 1         
##  Mcnemar&amp;#39;s Test P-Value : NA        
##                                     
##             Sensitivity : 1.000     
##             Specificity : 1.000     
##          Pos Pred Value : 1.000     
##          Neg Pred Value : 1.000     
##              Prevalence : 0.507     
##          Detection Rate : 0.507     
##    Detection Prevalence : 0.507     
##       Balanced Accuracy : 1.000     
##                                     
##        &amp;#39;Positive&amp;#39; Class : tail      
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An accuracy of 1. That is extremely high. Should we choose this one as our final model? Let’s build a couple other models first.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-include-all-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 2: Include all features!&lt;/h2&gt;
&lt;p&gt;Let’s change some hyper-parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treeCon &amp;lt;- rpart.control(minsplit = 10, minbucket = 3, cp = 0.01, xval = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have made each branching node and leaf larger, with also a higher quality improvement requirement during pruning. Let’s build the model now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree2 &amp;lt;- rpart(response ~ inst + volt + water, data = data.train, control = treeCon)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that although our hyper-parameters demand an overall decrease in structural complexity of the tree, we are now also using the other features Mike collected such as voltage and water pressure deviations to construct the model. Here’s the final strucutre:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, this is a much simpler tree in terms of sturctural complexity (albeit still pretty complex). However it does make use of all three features, some of them we suspect to be useless. What about its training set accuracy?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train2 &amp;lt;- predict(tree2, data.train, type = &amp;quot;class&amp;quot;)
conMat2 &amp;lt;- confusionMatrix(pred.train2, data.train$response)
conMat2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  213   58
##       head   92  238
##                                         
##                Accuracy : 0.75          
##                  95% CI : (0.714, 0.785)
##     No Information Rate : 0.507         
##     P-Value [Acc &amp;gt; NIR] : &amp;lt; 2e-16       
##                                         
##                   Kappa : 0.502         
##  Mcnemar&amp;#39;s Test P-Value : 0.00705       
##                                         
##             Sensitivity : 0.698         
##             Specificity : 0.804         
##          Pos Pred Value : 0.786         
##          Neg Pred Value : 0.721         
##              Prevalence : 0.507         
##          Detection Rate : 0.354         
##    Detection Prevalence : 0.451         
##       Balanced Accuracy : 0.751         
##                                         
##        &amp;#39;Positive&amp;#39; Class : tail          
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With an accuracy of 0.7504 it seems to be markedly worse than model 1…or is it? At the very least it still has quite a bit of predictive power since it’s accuracy is significantly higher than the &lt;em&gt;No Information Rate&lt;/em&gt;. Let’s build one final model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-simple-structure-with-only-the-relevant-feature.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 3: Simple structure with only the relevant feature.&lt;/h2&gt;
&lt;p&gt;No changes will be made to the hyper-parameters here. Instead, we will train the model with just one difference:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
tree3 &amp;lt;- rpart(response ~ inst, data = data.train, control = treeCon)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tree is built with the same complexity parameters as our last tree, but only using the instrument measurements as its feature. Let’s check out its structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(tree3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2017-06-30-overfitting_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly this is the simplest tree by far in terms of both structure and feature usage. How about its accuracy on the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.train3 &amp;lt;- predict(tree3, data.train, type = &amp;quot;class&amp;quot;)
conMat3 &amp;lt;- confusionMatrix(pred.train3, data.train$response)
conMat3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction tail head
##       tail  231  100
##       head   74  196
##                                         
##                Accuracy : 0.71          
##                  95% CI : (0.672, 0.746)
##     No Information Rate : 0.507         
##     P-Value [Acc &amp;gt; NIR] : &amp;lt;2e-16        
##                                         
##                   Kappa : 0.42          
##  Mcnemar&amp;#39;s Test P-Value : 0.0581        
##                                         
##             Sensitivity : 0.757         
##             Specificity : 0.662         
##          Pos Pred Value : 0.698         
##          Neg Pred Value : 0.726         
##              Prevalence : 0.507         
##          Detection Rate : 0.384         
##    Detection Prevalence : 0.551         
##       Balanced Accuracy : 0.710         
##                                         
##        &amp;#39;Positive&amp;#39; Class : tail          
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Uh oh. With an accuracy of 0.7105 this is by far the worst model we have built! I guess it’s clear we should use the big tree from model 1 as our final model, right? Oh wait, we split the data into training and validation set! Since the models are all built on the training set they have no ideas what the validation set looks like. If we feed the models the validation set we can make actual predictions with them and get a sense on its predictive accuracy. How exciting! Let’s do it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-evaluation-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance Evaluation (Validation)&lt;/h2&gt;
&lt;p&gt;Here’s the code to make predictions on the validation set and compute the prediction statistics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.validation.1 &amp;lt;- predict(tree1, data.validation, type = &amp;quot;class&amp;quot;)
pred.validation.2 &amp;lt;- predict(tree2, data.validation, type = &amp;quot;class&amp;quot;)
pred.validation.3 &amp;lt;- predict(tree3, data.validation, type = &amp;quot;class&amp;quot;)

confMat.valid.1 &amp;lt;- confusionMatrix(pred.validation.1, data.validation$response)
confMat.valid.2 &amp;lt;- confusionMatrix(pred.validation.2, data.validation$response)
confMat.valid.3 &amp;lt;- confusionMatrix(pred.validation.3, data.validation$response)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am just going to list the accuracies below in a table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(Model = c(&amp;quot;Model 1&amp;quot;, &amp;quot;Model 2&amp;quot;, &amp;quot;Model 3&amp;quot;), Accuracy = c(confMat.valid.1$overall[1], 
    confMat.valid.2$overall[1], confMat.valid.3$overall[1]))

kable(df)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6466&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6591&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6817&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Well well, the table has turned! The big tree in model1, although has an amazing training set accuracy, is now the definitive last place when it comes to making predictions on the validation set. The tree with the worst training set performance, the tree with low complexity parameters and only using the instrument measurement as its feature, is now the best performer. Why is that?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;By now you probably have a good idea to the answer already. The first two trees overfit the data (especially the first one). The first model contains too many nodes and leafs. The result is it is able to fit every single data point in the training set into single leafs by itself and simply look them up. No wonder it has such amazing (and grossly inflated) training set accuracy. However, when it comes to making actual predictions on the validation set all the noise throws it off and causes it to overreact on noisy signals that don’t have any real effect on the response. The result is terrible predictive accuracy.&lt;/p&gt;
&lt;p&gt;Although the second tree tries to avoid very complex structure, it uses features that are clearly useless in predicting the response (water pressure and voltage deviations…really, Mike?). The good thing is the pruning process is able to catch some of these and cut them off the trees so it doesn’t make too much of a negative impact on its prediction accuracy.&lt;/p&gt;
&lt;p&gt;Nevertheless, you can see that model 3, with the simplest structure and using only the relevant feature, turns out to be the best one in the end. This simple structure is achieved by &lt;em&gt;tuning hyper-parameters&lt;/em&gt; and using only relevant features aka &lt;em&gt;feature selection&lt;/em&gt;. In this case, they are largely done by experience and logic. I know beforehand that the data is noisy and two of the features are useless, so all I need to do is just selecting sensible model parameter values relative to my other choices and get rid of useless features. This is actually an &lt;em&gt;excellent&lt;/em&gt; way to build models as it is not done using information obtained directly from the data so there is no risk of overfitting.&lt;/p&gt;
&lt;p&gt;For a model builder, the biggest problem with overfitting is it contaminates model performance assessment which leads to making the wrong choice. Think back on the inflated training set accuracies. If I used the training set accuracy as the performance indicator I would have chosen the worst model in the end. Some may ask, what if I tweak model parameters and select features using the statistics produced by a previous model? The truth is, &lt;strong&gt;whenever you are using the data to gain information about what your model should be, what features you shoud select etc, you can potentially overfit&lt;/strong&gt;. For example, now I know the prediction accuracies of the three models on the validation set, if I go back, tweak some parameters/select some features and retrain the model on the training set to improve my prediction accuracy on the validation set, the validation set accuracy is likely going to be biased due to overfitting as well. Why? Because the accuracy assessment on the validation set is information gleaned from the validation set. By going back to make model tweaks based on it, information in the validation set is now spilling over to the model training process and the validation set accuracy will likely become biased just like the training set accuracies.&lt;/p&gt;
&lt;p&gt;Wait, does this mean we can only use the validation set to produce an accurate estimate of model performance ONCE? Well, technically, yes. However, there are techniques to repeatedly produce accurate performance estimates and avoid/delay bias caused by overfitting such as &lt;strong&gt;resampling&lt;/strong&gt;. We shall see this in another post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Poor Pigs Problem</title>
      <link>https://fengkehh.github.io/post/2017-04-26-the-poor-pigs-problem/</link>
      <pubDate>Sun, 30 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://fengkehh.github.io/post/2017-04-26-the-poor-pigs-problem/</guid>
      <description>&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;!-- BLOGDOWN-BODY-BEFORE --&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#additional-clarfications&#34;&gt;Additional Clarfications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-binary-state-method&#34;&gt;The Binary State Method&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#one-round-of-testing&#34;&gt;One Round of Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#four-rounds-of-testing&#34;&gt;Four Rounds of Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#back-to-the-original&#34;&gt;Back to the Original&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#follow-up&#34;&gt;Follow Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#digression&#34;&gt;Digression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!-- /BLOGDOWN-BODY-BEFORE --&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This is an interesting problem I ran across on &lt;a href=&#34;https://leetcode.com/problems/poor-pigs/#/description&#34;&gt;leetcode.com&lt;/a&gt;. I have included the description of it below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are 1000 buckets, one and only one of them contains poison, the rest are filled with water. They all look the same. If a pig drinks that poison it will die within 15 minutes. What is the minimum amount of pigs you need to figure out which bucket contains the poison within one hour.&lt;/p&gt;
&lt;p&gt;Answer this question, and write an algorithm for the follow-up general case.&lt;/p&gt;
&lt;p&gt;Follow-up:&lt;/p&gt;
&lt;p&gt;If there are n buckets and a pig drinking poison will die within m minutes, how many pigs (x) you need to figure out the “poison” bucket within p minutes? There is exact one bucket with poison.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;additional-clarfications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Additional Clarfications&lt;/h2&gt;
&lt;p&gt;I am going to work with the following details:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A pig has infinite speed when it comes to drinking from the buckets (ie: you can have one pig drink from all 1000 buckets in as short amount of time as you need and still receive a fatal dose).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a pig is poisoned it dies sometime &lt;em&gt;within&lt;/em&gt; 15 minutes instead of at exactly the 15-minute mark. This ensures that you cannot use the trivial solution where you find the poisoned bucket by getting only one pig to quickly drink all 1000 buckets and figure out the poisoned bucket by timing the pig’s death.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-binary-state-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Binary State Method&lt;/h1&gt;
&lt;div id=&#34;one-round-of-testing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One Round of Testing&lt;/h2&gt;
&lt;p&gt;Disclaimer: I didn’t come up with this entirely by myself. Credit should also go to &lt;a href=&#34;https://www.linkedin.com/in/ting-zheng-97a2267b/&#34;&gt;Ting Zheng&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First let’s start with a simplified case with a total number of n = 4 buckets and &lt;strong&gt;only one round of testing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Buckets: &lt;code&gt;A, B, C, D&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now take &lt;strong&gt;x = 2&lt;/strong&gt; pigs and divide the buckets as follow:&lt;/p&gt;
&lt;p&gt;P1: &lt;code&gt;A, B&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;P2: &lt;code&gt;B, C&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;No pigs: &lt;code&gt;D&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;No matter which bucket contains the poison you can always deduce it by looking at the combination of dead/alive states of P1 and P2 after 15 minutes. This works because the total number of combinations using the binary states of each pig is &lt;span class=&#34;math inline&#34;&gt;\(2^2 = 4 \ge n\)&lt;/span&gt; .&lt;/p&gt;
&lt;p&gt;This can be extended for a larger number of buckets as well, for example, for n = 16 buckets we can do it with 4 pigs:&lt;/p&gt;
&lt;p&gt;Buckets: &lt;code&gt;A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;P1: &lt;code&gt;A, B, C, D, E, F, G, H&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;P2: &lt;code&gt;E, F, G, H, I, J, K, L&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;P3: &lt;code&gt;A, B, E, F, I, J, M, N&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;P4: &lt;code&gt;B, C, F, G, J, K, N, O&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;No Pigs: &lt;code&gt;P&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here is a graphical representation of the above division of buckets to each pig:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../img/poor_pigs.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;You can see that the buckets are divided into repeating sub-units of 4 similar to the 4-bucket case. Once again, total number of dead/alive combinations is &lt;span class=&#34;math inline&#34;&gt;\(2^4 = 16 \ge n\)&lt;/span&gt;. In fact, for any n buckets you can find the poisoned one with a minimum of &lt;span class=&#34;math inline&#34;&gt;\(x = \lceil log_2(n) \rceil\)&lt;/span&gt; pigs (ie: the smallest x to satisfy &lt;span class=&#34;math inline&#34;&gt;\(2^x \ge n\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;four-rounds-of-testing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Four Rounds of Testing&lt;/h2&gt;
&lt;p&gt;Now we will consider the original question and allow &lt;strong&gt;four rounds&lt;/strong&gt; per the original requirement. Let’s start with just a generalized number of buckets, n. Since we have 4 tries, we can start by cutting down the number of buckets that we have to test in detail as much as possible. We can do this by dividing the buckets into groups and test each group with just one pig to locate the group that contains the poison.&lt;/p&gt;
&lt;p&gt;If we have x pigs in total, we can divide n buckets into (x + 1) groups (if no pigs die then it must be the group that isn’t tested). Since the question asks for a minimum x pigs that guarantees finding the poison, we need to be conservative here and consider only the worst case scenario where we lose as many pigs as possible. So in this first division we lose one pig and have (x - 1) pigs left.&lt;/p&gt;
&lt;p&gt;We then repeat the process twice. Since we have lost a pig already, we can only divide into x groups now (and lose another pig in the process). Finally on the third division we have (x - 1) groups and lose one last pig. We now have (x - 3) pigs left and a group with approximately &lt;span class=&#34;math inline&#34;&gt;\(\frac{n}{(x + 1)(x)(x - 1)}\)&lt;/span&gt; buckets. These are the buckets that we must test in detail on the last round using the binary state combinations of the (x - 3) pigs we have left in order to locate the poisoned bucket (at most &lt;span class=&#34;math inline&#34;&gt;\(2^{x - 3}\)&lt;/span&gt; buckets).&lt;/p&gt;
&lt;p&gt;Working backward, we can see that the minimum amount of pigs we need is the smallest x that satisfies the following:&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:maxbuckets&#34;&gt;\[\begin{equation}
2^{x-3}(x+1)(x)(x-1) \ge n
\tag{1}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;where x = number of pigs and n = number of buckets.&lt;/p&gt;
&lt;p&gt;Note that in order for eq &lt;a href=&#34;#eq:maxbuckets&#34;&gt;(1)&lt;/a&gt; to make sense we must start with at least 4 pigs (otherwise you will run out of pigs in the worst case scenario). This means this particular method is only justified for a total number of buckets exceeding 120. For buckets less than 120 you may want to reduce the number of rounds dedicated to dividing buckets into groups and avoid losing pigs unnecessarily.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-the-original&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to the Original&lt;/h2&gt;
&lt;p&gt;With n = 1000, the minimum x that satisfies equation &lt;a href=&#34;#eq:maxbuckets&#34;&gt;(1)&lt;/a&gt; is x = 6. In fact, with 6 pigs you can test up to a whopping &lt;strong&gt;&lt;em&gt;1680&lt;/em&gt;&lt;/strong&gt; buckets in an hour!&lt;/p&gt;
&lt;div id=&#34;follow-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Follow Up&lt;/h3&gt;
&lt;p&gt;If pigs die within m minutes and we are given p minutes, then the number of test rounds is &lt;span class=&#34;math inline&#34;&gt;\(\lfloor\frac{p}{m}\rfloor\)&lt;/span&gt;. Out of these, &lt;span class=&#34;math inline&#34;&gt;\(\lfloor\frac{p}{m}\rfloor - 1\)&lt;/span&gt; rounds are used to divide buckets into groups and zoom in on the general location of the poison. Equation &lt;a href=&#34;#eq:maxbuckets&#34;&gt;(1)&lt;/a&gt; can then be generalized to&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:generalized&#34;&gt;\[\begin{equation}
2^{x- \lfloor \frac{p}{m} \rfloor + 1}\prod_{i = 1}^{\lfloor \frac{p}{m} \rfloor - 1} (x + 2 - i) \ge n
\tag{2}
\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;digression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Digression&lt;/h1&gt;
&lt;p&gt;There is an interesting suggestion proposed by &lt;a href=&#34;https://discuss.leetcode.com/topic/67666/another-explanation-and-solution&#34;&gt;StephanPochmann&lt;/a&gt;. The suggestion essentially advocates arranging the buckets into 5x5 square matrices and use two pigs each to find the (x, y) coordinates of the poisoned bucket within four rounds of testing. If there are more than 25 buckets in total you can divide the buckets into units of 5x5 matrices and assign two pigs for each. It further postulates an extension of using 5 x 5 x 5 cubes and 3 pigs per cube. However, there are some problems:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;While the method using square matrices is possible, the suggestion of further optimization using cubes is not. Suppose there is exactly one 5 x 5 x 5 cube of buckets and 3 pigs total for the 3 axis. Each pig can drink a max of 5 buckets on their respective axis. With 3 pigs you get 15 buckets tested per round. After 4 rounds you have only tested 15 x 4 = 60 buckets and that is assuming no buckets are tested multiple times. That leaves at least 65 buckets untested in each cube! If you want guaranteed results you can only leave at most &lt;strong&gt;1&lt;/strong&gt; bucket untested out of all of the cubes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the total number of buckets requires more than one matrix units you cannot systematically leave one bucket untested in each matrix because again, only one untested buckets out of all of the matrices combined is allowed for guaranteed result. Therefore in the cases where you have more than one matrix unit each matrix can only be a 5 x 5 minus one of its corners (24 buckets in total) with the exception of a single full 5 x 5 matrix at the beginning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assuming p = 60 and m = 15 as per the original specification, since the number of buckets tested using the binary state method grows exponentially by the number of pigs, it quickly overtakes the matrix method in terms of efficiency as the number of buckets increases. Specifically, The matrix method can test 16 buckets using two pigs. Any n &amp;gt; 25 requires you to divide them into matrix groups. This means x is roughly &lt;span class=&#34;math inline&#34;&gt;\(2*\frac{n}{24}\)&lt;/span&gt; -&amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(O(n)\)&lt;/span&gt;. In comparison, the binary method is actually better than &lt;span class=&#34;math inline&#34;&gt;\(O(log(n))\)&lt;/span&gt; (see &lt;a href=&#34;#eq:maxbuckets&#34;&gt;(1)&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making A Blog Site with RStudio, R Markdown and Blogdown Part II</title>
      <link>https://fengkehh.github.io/post/2017-03-16-how-this-blog-is-made-part-ii/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://fengkehh.github.io/post/2017-03-16-how-this-blog-is-made-part-ii/</guid>
      <description>&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;!-- BLOGDOWN-BODY-BEFORE --&gt;
&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#blogdown-and-hugo&#34;&gt;Blogdown and Hugo&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#github-pages&#34;&gt;Github (Pages)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#setup-1&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#site-creation-submodule&#34;&gt;Site Creation &amp;amp; Submodule&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#setup-2&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#site-configuration-and-deployment&#34;&gt;Site Configuration and Deployment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!-- /BLOGDOWN-BODY-BEFORE --&gt;
&lt;p&gt;This is the continuation of &lt;a href=&#34;../2017-03-15-how-this-blog-is-made/&#34;&gt;Part I&lt;/a&gt;. In this section I am going to cover setting up &lt;em&gt;Blogdown&lt;/em&gt;, &lt;em&gt;Hugo&lt;/em&gt; and &lt;em&gt;Github Pages&lt;/em&gt; to start blogging. This is essentially a mix of the excellent guides by &lt;a href=&#34;https://proquestionasker.github.io/blog/Making_Site/&#34;&gt;Amber Thomas&lt;/a&gt;, &lt;a href=&#34;https://hjdskes.github.io/blog/update-deploying-hugo-on-personal-gh-pages/&#34;&gt;Jente Hidske&lt;/a&gt; and &lt;a href=&#34;http://robertmyles.github.io/2017/02/01/how-to-make-a-github-pages-blog-with-rstudio-and-hugo/&#34;&gt;Robert McDonnell&lt;/a&gt;. However if you want a one stop solution with some of my personal twists, here it is!&lt;/p&gt;
&lt;p&gt;I am going to assume you are using Microsoft Windows because that is what I use. The process should be largely the same for Linux and Mac OSX but you may need to install extra softwares such as &lt;em&gt;Homebrew&lt;/em&gt;. If you see an item like &lt;em&gt;&amp;lt; something &amp;gt;&lt;/em&gt;, that means replace it with whatever &lt;em&gt;something&lt;/em&gt; indicates (ie: your username, directory name etc etc) minus the angle brackets. Once you finished every step you should be able to access your online website at &lt;code&gt;https://&amp;lt;your username&amp;gt;.github.io/&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;blogdown-and-hugo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Blogdown and Hugo&lt;/h2&gt;
&lt;p&gt;Hugo is a static website delivery system. It is very easy to create a website with consistent theme and styling using Hugo. The only problem for us is that Hugo does NOT inherently support R Markdown. This is why we need Blogdown, the brainchild of Yihui Xie (aka Emperor of R Markdown, First of His Name), to bridge the water.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Launch RStudio. Type the following command to install Blogdown and load it into R.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;#39;httpuv&amp;#39;, &amp;#39;devtools&amp;#39;))
devtools::install_github(&amp;quot;rstudio/blogdown&amp;quot;)
library(blogdown)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Install Hugo using the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install_hugo()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Right now we can actually start generating a new site. However it is a good time to go set up our webhost first.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;github-pages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Github (Pages)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com&#34;&gt;Github&lt;/a&gt; is an online repository site utilizing the Git technology. Normally Github is used for version control in software development. However it has a lesser known functionality called Github Pages which is essentially free webhosting for static websites.&lt;/p&gt;
&lt;p&gt;Here’s a bit of tangent on how Github works. &lt;strong&gt;Skip straight to Setup if you are not interested.&lt;/strong&gt; On Github you have repositories. Think of them like document folders for different projects. Your Github account is essentially an online desk where you keep all of these folders/repositories inside and the underlying &lt;em&gt;git&lt;/em&gt; version control software is like the secretary. When you want to take a look at a particular project, you tell &lt;strong&gt;git&lt;/strong&gt; to give you a copy/&lt;strong&gt;clone&lt;/strong&gt; of the repository containing that project to keep on your own desk. This then becomes your local repository.&lt;/p&gt;
&lt;p&gt;You might then make some changes and add different things to the local repository. As you make changes you can make snapshots of your progress by telling git to &lt;strong&gt;commit&lt;/strong&gt; changes so you can revert to this point if you mess up later. Kind of like creating save files when you are playing a video game. Note that up to now all of your changes only happen in your local repository. Once you are really done for the night you can tell git to &lt;strong&gt;push&lt;/strong&gt; the changes to Github and git will make all the changes to the online repository to match your local repository.&lt;/p&gt;
&lt;p&gt;Github Pages is basically a special repository for you. Github will render html files inside this repository as webpages if someone types out the correponding url in a web browser. This means you can use this repository as a personal webhost and maintain it using git. That is what we are trying to achieve.&lt;/p&gt;
&lt;div id=&#34;setup-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;p&gt;We are going to set everything up for a technique called &lt;em&gt;submodule&lt;/em&gt;. I will explain the reason later. In the mean time you should follow the steps below &lt;strong&gt;EXACTLY&lt;/strong&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Sign up for a (free) account on &lt;a href=&#34;https://github.com&#34;&gt;Github&lt;/a&gt;. Choose a reasonable username because that will be part of the URL pointing to your site.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Download and install &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;Git&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Configure it for RStudio by clicking on &lt;strong&gt;Tools&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; &lt;strong&gt;Global Options&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; &lt;strong&gt;Git/SVN&lt;/strong&gt;. Set the path to the git executable (default: &lt;code&gt;C:/Program Files/Git/bin/git.exe&lt;/code&gt;). Check the box for Using Git Bash.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;strong&gt;Create RSA Key…&lt;/strong&gt;. Leave everything at the default values (passphrase and confirm are blank) and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;strong&gt;View public key&lt;/strong&gt; and copy the text into clipboard.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../img/rstudio_ssh_key.png&#34; /&gt;

&lt;/div&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to &lt;a href=&#34;https://github.com&#34;&gt;Github&lt;/a&gt;, click on your profile on the top right and select &lt;strong&gt;Settings&lt;/strong&gt;. Click on &lt;strong&gt;SSH and GPG Keys&lt;/strong&gt; on the left and press the green &lt;strong&gt;New SSH key&lt;/strong&gt; button. Type whatever name you want for the title and paste the block of text you got from the last step into the &lt;strong&gt;Key&lt;/strong&gt; textfield. Add the key.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Creating Github Pages repository:&lt;/strong&gt; Go to Github. Click on New Repository. Select &lt;strong&gt;Initialize this repository with a README&lt;/strong&gt; so your repository comes with a README.md file. Name this repository &lt;em&gt;&amp;lt;github username&amp;gt;.github.io&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../img/github_repo.png&#34; /&gt;

&lt;/div&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Creating source repository:&lt;/strong&gt; Go back to the front page for Github and make another repository. The name doesn’t matter. I named mine &lt;em&gt;blog-source&lt;/em&gt;. However make sure you &lt;strong&gt;deselect&lt;/strong&gt; “Initialize this repository with a README”. Once the repository is created you should see a URL to this repository under Quick Setup in the form of &lt;code&gt;https://github.com/&amp;lt;github username&amp;gt;/&amp;lt;source repository name&amp;gt;.git&lt;/code&gt; Copy this into clipboard.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Launch RStudio. Click &lt;strong&gt;File&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; &lt;strong&gt;New Project…&lt;/strong&gt; and select &lt;strong&gt;Version Control&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; &lt;strong&gt;Git&lt;/strong&gt;. Paste the URL for your &lt;strong&gt;source repository&lt;/strong&gt; under Repository URL. Choose a location to create your local repository under and click Create Project. RStudio will automatically switch to your local respository afterwards.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are now ready to generate our new site and configure it to use Github Pages as the webhost.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;site-creation-submodule&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Site Creation &amp;amp; Submodule&lt;/h2&gt;
&lt;p&gt;Here’s a bit of explanation on what we did before. Again, &lt;strong&gt;skip to Setup if you are not interested&lt;/strong&gt;. The reason we are using two repositories is because Hugo generates the website into a subdirectory called &lt;code&gt;public&lt;/code&gt;. Unfortunately, Github Pages for personal websites only serves homepage from the root of its master branch. To work around this we are going to link our &lt;code&gt;public&lt;/code&gt; directory in the local repository to our Github Pages repository while the root directory of our local repository remains linked to the source repository. Thus, if we commit then push changes in the root of our local repository, all the changes will go to the source repository. If we commit and push changes in the public subdirectory though, they will instead go into the Github Pages repo. As I have foreshadowed, this is realized using &lt;em&gt;submodule&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;setup-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In RStudio, open the project we just created and make sure Blogdown is loaded using &lt;code&gt;library(&#39;blogdown&#39;)&lt;/code&gt;. Type &lt;code&gt;new_site()&lt;/code&gt; to create a blank site with the default theme. The homepage will show up in viewer. Click the red stop button to quit the viewer and go back to console.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Launch Git Bash terminal.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../img/git_shell.png&#34; /&gt;

&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In the terminal, you should be in the directory for your local source repository. There should be a subdirectory called &lt;strong&gt;public&lt;/strong&gt;. Delete it with the command &lt;code&gt;rm -rf public&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the terminal, type the following command (one single line!) to clone your Github Pages repository into a subdirectory called &lt;code&gt;public&lt;/code&gt; using &lt;em&gt;submodule&lt;/em&gt;. Remember to replace with your own username.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;git submodule add -b master git@github.com:&amp;lt;username&amp;gt;/&amp;lt;username&amp;gt;.github.io.git public&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Double check by typing &lt;code&gt;git remote show origin&lt;/code&gt; in the terminal under the root directory of your local source repository. The origin should point to your &lt;strong&gt;online source repository&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type &lt;code&gt;cd public&lt;/code&gt; and &lt;code&gt;git remote show origin&lt;/code&gt; again. This time it should point to your &lt;strong&gt;Github Pages repository&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In RStudio, you should see a file called .gitignore in RStudio’s file browser. Click on it to edit it. Add a new line &lt;code&gt;public/&lt;/code&gt; at the bottom. Save the file.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;site-configuration-and-deployment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Site Configuration and Deployment&lt;/h2&gt;
&lt;p&gt;Time to configure our site and deploy it for the first time!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The default theme is a bit barebones so you might want to install a new theme using the following command in RStudio with blogdown loaded:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install_theme(&amp;#39;&amp;lt;creator github name&amp;gt;/&amp;lt;theme name&amp;gt;&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, this blog site uses the &lt;a href=&#34;https://github.com/Vimux/Mainroad&#34;&gt;Mainroad&lt;/a&gt; theme. So I typed &lt;code&gt;install_theme(&#39;Vimux/Mainroad&#39;)&lt;/code&gt; to install it. Notice that there is no leading or trailing &lt;code&gt;/&lt;/code&gt;. After installation, open &lt;code&gt;config.toml&lt;/code&gt; in your root folder and double check that the variable &lt;code&gt;theme&lt;/code&gt; has been set to your theme of choice.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To use &lt;span class=&#34;math inline&#34;&gt;\(\LaTeX\)&lt;/span&gt; expressions you must enable the &lt;em&gt;mathjax&lt;/em&gt; javascript. I adopted YiHui Xie’s &lt;a href=&#34;https://github.com/yihui/hugo-lithium-theme/blob/7d436d803df90c873cdaecf24aeeff827696d77c/layouts/partials/footer.html#L21-L30&#34;&gt;approach&lt;/a&gt;. Copy the highlighted code chunk and insert it into a layout specification that you are sure to be loaded on every page by your theme. I inserted it into the footer.html file located under &lt;code&gt;themes/&amp;lt;theme name&amp;gt;/layouts/partials/&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are other self-explanatory settings in &lt;strong&gt;config.toml&lt;/strong&gt; that you should change. For example, title, baseurl, author name and descriptions, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use &lt;code&gt;new_post(&#39;&amp;lt;title&amp;gt;&#39;)&lt;/code&gt; to create a new blog post as a .Rmd file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want a preview of your site, type &lt;code&gt;serve_site()&lt;/code&gt; in RStudio to start a local server with a copy of your website on display.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you are satisfied, build the online version of the site using the &lt;code&gt;build_site()&lt;/code&gt; command in RStudio. After that you can use git to push the web pages inside your &lt;strong&gt;public/&lt;/strong&gt; folder into the Github Pages repository. I have modified &lt;a href=&#34;https://hjdskes.github.io/blog/update-deploying-hugo-on-personal-gh-pages/&#34;&gt;Jente Hidske&lt;/a&gt;’s shell script into a R script to streamline deployment:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Customized R function to facilitate fast site deployment
require(blogdown)

deploy &amp;lt;- function() {
    
    # Make sure things are commited
    output &amp;lt;- system(&amp;#39;git status -s&amp;#39;, intern = TRUE)
    
    if (length(output)) {
        print(&amp;#39;Dirty work directory. Commit/revert changed files first.&amp;#39;)
    } else {
        # Remove old website
        unlink(list.files(path = &amp;#39;./public&amp;#39;, full.names = TRUE), 
               recursive = TRUE)
        
        # Build website from source
        build_site()
        
        # Push site
        message &amp;lt;- paste(&amp;#39;Site rebuild&amp;#39;, as.character(Sys.time()))
        setwd(&amp;#39;./public&amp;#39;)
        
        fileConn&amp;lt;-file(&amp;quot;deploy.sh&amp;quot;)
        writeLines(c(&amp;#39;git add -A&amp;#39;,
                     paste(&amp;#39;git commit -m&amp;#39;, &amp;#39; \&amp;quot;&amp;#39;, message, &amp;#39;\&amp;quot;&amp;#39;, sep = &amp;#39;&amp;#39;), 
                     &amp;#39;git push&amp;#39;), 
                   fileConn)
        close(fileConn)
        
        setwd(&amp;#39;..&amp;#39;)
        print(message)
        print(&amp;#39;Go to ./public and execute ./deploy.sh&amp;#39;)
        
    }
    
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you copy the above code into an empty .R script file, save it as deploy.R and use &lt;code&gt;source(&#39;deploy.R&#39;)&lt;/code&gt; to load the script into memory. Whenever you want to deploy your site, first commit your changes to the local source repository (ie: by RStudio Git GUI or command line), then type &lt;code&gt;deploy()&lt;/code&gt; in RStudio to build the site. Once that is done, go to the terminal and type &lt;code&gt;cd public&lt;/code&gt; to go to the Github Pages local repository. Type &lt;code&gt;./deploy.sh&lt;/code&gt; to complete deployment.&lt;/p&gt;
&lt;p&gt;I also recommend that you create a file named &lt;strong&gt;.Rprofile&lt;/strong&gt; in the root of your local source repository. Inside I put common commands that I want RStudio to run every time it loads the project for the website. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Required library/files
library(blogdown)
source(&amp;#39;deploy.R&amp;#39;)

# Default Options
options(blogdown.author = &amp;#39;&amp;lt;your name here&amp;gt;&amp;#39;)
options(blogdown.rmd = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making A Blog Site with RStudio, R Markdown and Blogdown Part I</title>
      <link>https://fengkehh.github.io/post/2017-03-15-how-this-blog-is-made/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://fengkehh.github.io/post/2017-03-15-how-this-blog-is-made/</guid>
      <description>&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;!-- BLOGDOWN-BODY-BEFORE --&gt;
&lt;!-- /BLOGDOWN-BODY-BEFORE --&gt;
&lt;p&gt;This blog is made of one philosophy resting on four pillars: it should cost exactly nothing and all the software you need are the following.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The R programming language for data analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The R Markdown language for content generation and typesetting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Blogdown in conjunction with Hugo to generate the web pages.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Github-Pages for webhosting.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This post will mainly touch on 1 and 2. &lt;a href=&#34;../2017-03-16-how-this-blog-is-made-part-ii/&#34;&gt;Part II&lt;/a&gt; will focus on 3 and 4 so you can get your own blog up and running if you want to follow in my footsteps.&lt;/p&gt;
&lt;div id=&#34;the-r-programming-language&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The R Programming Language&lt;/h2&gt;
&lt;p&gt;R is a programming language designed for statistical computation. It has very powerful data manipulation and 2d plotting capabilities. However to use it to its full potential you need to let go of graphical user interfaces and start writing R scripts. If you have always relied on GUI before, you should know that having to code everything is a good thing! Imagine having to explain to someone which buttons to click and how far he has to move the slider to get the same result you have. If you did everything in R all you need to do is just showing them the code.&lt;/p&gt;
&lt;p&gt;To get R up and running on your computer, download and install the R distribution for your operating system from the &lt;a href=&#34;https://cran.r-project.org/&#34;&gt;official website&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-markdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Markdown&lt;/h2&gt;
&lt;p&gt;Perhaps the most important reason to create a blog site this way is none other than R Markdown. You can think of R Markdown as a language that allows you to quickly generate decent looking webpages with embedded R codes, graphics and even &lt;span class=&#34;math inline&#34;&gt;\(\LaTeX\)&lt;/span&gt; expressions for mathematical equations. It is an incredibly efficient language in the sense that almost every single thing you type goes directly to the content you want to show to your audience as opposed to tweaking various display or formatting parameters.&lt;/p&gt;
&lt;p&gt;I use the popular &lt;a href=&#34;https://www.rstudio.com/products/rstudio/#Desktop&#34;&gt;RStudio&lt;/a&gt; to write R Markdown files. Be sure to install R first if you haven’t done so already.&lt;/p&gt;
&lt;p&gt;Once you have RStudio installed go ahead and launch it. Type the following lines in the console to install all the packages you need for R Markdown:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;#39;rmarkdown&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I won’t go into how to use RStudio or write R Markdown files here because there are many excellent &lt;a href=&#34;https://www.r-bloggers.com/r-markdown-and-knitr-tutorial-part-1/&#34;&gt;tutorials&lt;/a&gt; on the subject already. The reason I bring them up is because there are nice tools out there that help us convert content generated by R Markdown to content readable on a blog like this one. &lt;a href=&#34;../2017-03-16-how-this-blog-is-made-part-ii/&#34;&gt;Part II&lt;/a&gt; will talk about how to set up free webhosting using Github-Pages and configure Blogdown and Hugo to generate the static webpages for the blog site.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>